---
layout: page
title: Papers
description: >-
    Papers for consideration.
nav_order: 5
has_children: false
has_toc: true
---

# Research Papers
{:.no_toc}

## Table of contents
{: .no_toc .text-delta }

1. TOC
{:toc}

---

## RGB-D Architectures

Depth for object recognition:

- [3D ShapeNets: A Deep Representation for Volumetric Shapes](https://arxiv.org/abs/1406.5670){:target="_blank"}, Wu et al., 2015

- [VoxNet: A 3D Convolutional Neural Network for Real-Time Object Recognition](https://graphics.stanford.edu/courses/cs233-21-spring/ReferencedPapers/voxnet_07353481.pdf){:target="_blank"}, Maturana et al., 2015

RGB for pose estimation & Depth for pose refinement:
- [PoseCNN: A Convolutional Neural Network for 6D Object Pose Estimation in Cluttered Scenes](https://arxiv.org/abs/1711.00199){:target="_blank"}, Xiang et al., 2018

Unified RGB-D for pose estimation:
- [A Unified Framework for Multi-View Multi-Class Object Pose Estimation](https://arxiv.org/abs/1803.08103){:target="_blank"}, Li et al., 2018


Beyond single view:

- [Multi-view Convolutional Neural Networks for 3D Shape Recognition](https://openaccess.thecvf.com/content_iccv_2015/papers/Su_Multi-View_Convolutional_Neural_ICCV_2015_paper.pdf){:target="_blank"}, Su et al., 2015

- [Volumetric and Multi-View CNNs for Object Classification on 3D Data](https://openaccess.thecvf.com/content_cvpr_2016/papers/Qi_Volumetric_and_Multi-View_CVPR_2016_paper.pdf){:target="_blank"}, Qi et al., 2016


## Pointcloud Processing 

- [PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation](https://arxiv.org/abs/1612.00593){:target="_blank"}, Qi et al., 2017


- [PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space](https://arxiv.org/abs/1706.02413){:target="_blank"}, Qi et al., 2017


- [PointFusion: Deep Sensor Fusion for 3D Bounding Box Estimation](https://openaccess.thecvf.com/content_cvpr_2018/papers/Xu_PointFusion_Deep_Sensor_CVPR_2018_paper.pdf){:target="_blank"}, Xu et al., 2018

- [DenseFusion: 6D Object Pose Estimation by Iterative Dense Fusion](https://openaccess.thecvf.com/content_CVPR_2019/papers/Wang_DenseFusion_6D_Object_Pose_Estimation_by_Iterative_Dense_Fusion_CVPR_2019_paper.pdf){:target="_blank"}, Wang et al., 2019


## Object Pose, Geometry, SDF, Implicit surfaces

- [DeepSDF: Learning Continuous Signed Distance Functions for Shape Representation](https://openaccess.thecvf.com/content_CVPR_2019/papers/Park_DeepSDF_Learning_Continuous_Signed_Distance_Functions_for_Shape_Representation_CVPR_2019_paper.pdf){:target="_blank"}, Park et al., 2019

- [Implicit surface representations as layers in neural networks](https://openaccess.thecvf.com/content_ICCV_2019/papers/Michalkiewicz_Implicit_Surface_Representations_As_Layers_in_Neural_Networks_ICCV_2019_paper.pdf){:target="_blank"}, Michalkiewicz et al., 2019

- [Local Deep Implicit Functions for 3D Shape](https://arxiv.org/abs/1912.06126){:target="_blank"}, Genova et al., 2020

- [Implicit geometric regularization for learning shapes](https://arxiv.org/abs/2002.10099){:target="_blank"}, Gropp et al., 2020


## Dense object descriptors, Category-level representations

- [Normalized Object Coordinate Space for Category-Level 6D Object Pose and Size Estimation](https://geometry.stanford.edu/projects/NOCS_CVPR2019/){:target="_blank"}, Wang et al., 2019

- [kPAM: KeyPoint Affordances for Category-Level Robotic Manipulation](https://arxiv.org/abs/1903.06684){:target="_blank"}, Manuelli et al., 2019

- [Visual Descriptor Learning from Monocular Video](https://arxiv.org/abs/2004.07007){:target="_blank"}, Deekshith et al., 2020

- [Fully Self-Supervised Class Awareness in Dense Object Descriptors](https://proceedings.mlr.press/v164/hadjivelichkov22a.html){:target="_blank"}, Hadjivelichkov et al., 2022

- [Single-Stage Keypoint-Based Category-Level Object Pose Estimation from an RGB Image](https://arxiv.org/abs/2109.06161){:target="_blank"}, Lin et al., 2022 


## Recurrent Networks and Object Tracking

- [PoseRBPF: A Rao-Blackwellized Particle Filter for 6D Object Pose Tracking](https://arxiv.org/abs/1905.09304){:target="_blank"}, Deng et al., 2019



## Visual Odometry and Localization

- [Backprop KF: Learning Discriminative Deterministic State Estimators](https://proceedings.neurips.cc/paper/2016/file/697e382cfd25b07a3e62275d3ee132b3-Paper.pdf){:target="_blank"}, Haarnoja et al., 2016

- [Differentiable Particle Filters: End-to-End Learning with Algorithmic Priors](http://www.roboticsproceedings.org/rss14/p01.pdf){:target="_blank"}, Jonschkowski et al., 2018

- [Differentiable Algorithm Networks for Composable Robot Learning](https://arxiv.org/pdf/1905.11602.pdf){:target="_blank"}, Karkus et al., 2019

- [Chasing Ghosts: Instruction Following as Bayesian State Tracking](https://proceedings.neurips.cc/paper/2019/file/82161242827b703e6acf9c726942a1e4-Paper.pdf){:target="_blank"}, Anderson et al., 2019

- [Differentiable SLAM-net: Learning Particle SLAM for Visual Navigation](https://openaccess.thecvf.com/content/CVPR2021/papers/Karkus_Differentiable_SLAM-Net_Learning_Particle_SLAM_for_Visual_Navigation_CVPR_2021_paper.pdf){:target="_blank"}, Karkus et al., 2021



## Semantic Scene Graphs and Explicit Representations

- [Image Retrieval using Scene Graphs](https://openaccess.thecvf.com/content_cvpr_2015/papers/Johnson_Image_Retrieval_Using_2015_CVPR_paper.pdf){:target="_blank"}, Johnson et al., 2015

- [Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations](https://arxiv.org/abs/1602.07332){:target="_blank"}, Krishna et al., 2016

- [Image Generation from Scene Graphs](https://openaccess.thecvf.com/content_cvpr_2018/papers/Johnson_Image_Generation_From_CVPR_2018_paper.pdf){:target="_blank"}, Johnson et al., 2018

- [Differentiable Scene Graphs](https://arxiv.org/abs/1902.10200){:target="_blank"}, Raboh et al., 2020

- [Semantic Linking Maps for Active Visual Object Search](https://arxiv.org/abs/2006.10807){:target="_blank"}, Zeng et al., 2020


## Neural Radiance Fields and Implicit Representations

- [Scene Representation Networks: Continuous 3D-Structure-Aware Neural Scene Representations](https://arxiv.org/abs/1906.01618){:target="_blank"}, Sitzmann et al., 2019

- [Local Implicit Grid Representations for 3D Scenes](https://arxiv.org/abs/2003.08981){:target="_blank"}, Jiang et al., 2020

- [Convolutional occupancy networks](https://arxiv.org/abs/2003.04618){:target="_blank"}, Peng et al., 2020

- [NeRF Explosion 2020](https://dellaert.github.io/NeRF/){:target="_blank"}, Dellaert, 2020

- [NeRF-Supervision: Learning Dense Object Descriptors from Neural Radiance Fields](https://yenchenlin.me/nerf-supervision/){:target="_blank"}, Yen-Chen et al., 2022


## Datasets

RGB-D Datasets:

- [(NYU Depth v2) Indoor Segmentation and Support Inference from RGBD Images](https://cs.nyu.edu/~silberman/papers/indoor_seg_support.pdf){:target="_blank"}, Silberman et al., 2012

- [SUN RGB-D: A RGB-D Scene Understanding Benchmark Suite](https://openaccess.thecvf.com/content_cvpr_2015/papers/Song_SUN_RGB-D_A_2015_CVPR_paper.pdf){:target="_blank"}, Song et al., 2015

- [YCB-Video Dataset](https://arxiv.org/abs/1711.00199){:target="_blank"}, Xiang et al., 2018

- [BOP: Benchmark for 6D Object Pose Estimation](https://bop.felk.cvut.cz/home/){:target="_blank"}, Hoda≈à et al., 2019

- [ScanNet: Richly-annotated 3D Reconstructions of Indoor Scenes](http://www.scan-net.org){:target="_blank"}, Dai et al., 2019

- [TO-Scene: A Large-scale Dataset for Understanding 3D Tabletop Scenes](https://arxiv.org/abs/2203.09440), Xu et al., 2022

Semantic Datasets:

- [Habitat-Matterport 3D Semantics Dataset](https://arxiv.org/abs/2210.05633){:target="_blank"}, Yadav et al., 2022 

Object Model Datasets:

- [ShapeNet: An Information-Rich 3D Model Repository](https://shapenet.org){:target="_blank"}, Chang et al., 2015


Simulators:

- [MuJoCo: A physics engine for model-based control](https://ieeexplore.ieee.org/abstract/document/6386109){:target="_blank"}, Todorov et al., 2015

- [Pybullet, a python module for physics simulation for games, robotics and machine learning](https://pybullet.org/wordpress/){:target="_blank"}, Coumans et al., 2015

- [NVIDIA Isaac Sim](https://developer.nvidia.com/isaac-sim){:target="_blank"}

- [Isaac Gym: High Performance GPU-Based Physics Simulation For Robot Learning](https://arxiv.org/abs/2108.10470){:target="_blank"}, Makoviychuk et al., 2021


## Self-Supervised Learning

- [VICRegL: Self-Supervised Learning of Local Visual Features](https://arxiv.org/abs/2210.01571){:target="_blank"}, Bardes et al., 2022

- [Self-Supervised Geometric Correspondence for Category-Level 6D Object Pose Estimation in the Wild](https://kywind.github.io/self-pose){:target="_blank"}, Zhang et al., 2022



## Grasp Pose Detection

- [Self-Supervised Geometric Correspondence for Category-Level 6D Object Pose Estimation in the Wild](https://kywind.github.io/self-pose){:target="_blank"}, Zhang et al., 2022


## Transformer Architectures

- [SORNet: Spatial Object-Centric Representations for Sequential Manipulation](https://arxiv.org/abs/2109.03891){:target="_blank"}, Yuan et al., 2022

- [RT-1: Robotics Transformer for Real-World Control at Scale](https://robotics-transformer.github.io/assets/rt1.pdf){:target="_blank"}, Brohan et al., 2022

- [Transformers are Adaptable Task Planners](https://arxiv.org/abs/2207.02442){:target="_blank"}, Jain et al., 2022


## More Frontiers

Articulated and Deformable Objects:

- [Differentiable Nonparametric Belief Propagation](https://arxiv.org/abs/2101.05948){:target="_blank"}, Opipari et al., 2021

- [NARF22: Neural Articulated Radiance Fields for Configuration-Aware Rendering](https://arxiv.org/abs/2210.01166){:target="_blank"}, Lewis et al., 2022


Transparent Objects:

- [ClearPose: Large-scale Transparent Object Dataset and Benchmark](https://arxiv.org/abs/2203.03890), Chen et al., 2022

- [TransNet: Category-Level Transparent Object Pose Estimation](https://arxiv.org/abs/2208.10002){:target="_blank"}, Zhang et al., 2022

Dynamic Scenes:

- [D-NeRF: Neural Radiance Fields for Dynamic Scenes](https://arxiv.org/abs/2011.13961){:target="_blank"}, Pumarola et al., 2020




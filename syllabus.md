---
layout: page
title: Syllabus
description: >-
    Course policies and information.
nav_order: 2
---

# Course Syllabus
{:.no_toc}

## Table of contents
{: .no_toc .text-delta }

1. TOC
{:toc}

---

## About

Robots need to see and understand their world to be able to interact with objects and perform useful tasks autonomously. Perception is the essential first step in the process for endowing robots to perform autonomously.  Autonomous robots need to make sense of their sensory observations to represent the world around them – and enable their reasoning and action to a goal. Visual perception with cameras as sensors has matured due to the recent advancements in neural networks – which is especially true for performing visual recognition tasks such as object classification, detection, pose estimation, grasp pose detection, etc. 

This course aims to cover the necessary background of neural-network-based deep learning for robot perception – building on advancements in computer vision and enabling – for enabling robots to dexterously manipulate physical objects. During the first part of this course, students will learn to implement, train and debug their own neural networks. During the second part of this course, students will explore recent emerging topics in deep learning for robot perception and manipulation.  This exploration will include analysis of research publications in the area, building up to reproducing one of these publications for implementation as a final course project.

This course builds on (as a star and fork in the open-source sense) from these existing courses:
- [University of Michigan - EECS 498-007 / 598-005: Deep Learning for Computer Vision](https://web.eecs.umich.edu/~justincj/teaching/eecs498/WI2022/schedule.html){:target="_blank"}
- [Stanford - CS231n: Deep Learning for Computer Vision](http://cs231n.stanford.edu/index.html){:target="_blank"}


## Topics and Course Structure

The first half of the course will cover deep learning fundamentals in computer vision catered to robot perception problems.

- Linear classifiers
- Stochastic gradient descent
- Fully-connected networks
- Convolutional networks
- Pose estimation

The second half of the course will switch to seminar style covering following advanced topics in robot perception and manipulation via discussing publications.

- 3D vision in robotics
- Object perception for robot manipulation
- Neural radiance fields for perception
- Robot grasp pose detection

## Prerequisites

 - Strongly encouraged prerequisites:
   - Programming: ROB 320, EECS 281, or equivalent
   - Linear Algebra: ROB 101, MATH 214, MATH 217, or equivalent
 - Recommended prerequisites:
   - Prior experience with the [Python programming language](https://www.python.org/) is recommended.
   - Familiarity with gradients and how to calculate them from vector calculus.
   - Familiarity with random variables and probability distributions from probability theory.
   - Familiarity with concepts from machine learning (e.g. EECS 445) will be helpful.


<!-- ## Lectures

## Discussion Sections -->

## Grading Policy

 - Quizzes:  16x1%=16%  
 - Project 0:     12%
 - Project 1:     12%
 - Project 2:     12%
 - Project 3:     12%
 - Project 4:     12%
 - Final Project: 24%

<!-- ## Project List

- Project 0 (Week 1): Python refresher and warmup
- Project 1 (Weeks 2-3): Intro to pytorch, KNN classifier
- Project 2 (Weeks 4-5): Linear layers, two-layer classifier
- Project 3 (Weeks 6-7): Fully connected neural networks, convolutional neural networks
- Project 4 (Weeks 8-10): Autograd, autoencoder, pose estimation
- Final project (Weeks 11-13): Research paper reproduction, opetions include:
  - [INeRF](https://arxiv.org/abs/2012.05877)
  - [DPF: Differentiable Particle Filter](https://arxiv.org/abs/1805.11122)
  - [GDP: Grasp Pose Detection](https://arxiv.org/abs/1706.09911)
  - [Implicit Fields for manipulation](https://imrss2022.github.io)
- Final project (Weeks 14-15): Research paper extension and report

 -->